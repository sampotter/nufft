\section{The Fast Multipole Method}

The acceleration of the interpolation steps in the preceding section
relies on a sufficient accurate and optimized fast multipole method
(FMM) algorithm. The FMM is an algorithm which computes matrix-vector
products of a certain form in sub-quadratic
time~\cite{fmm-orig}. Typically, the FMM is developed for a particular
radial basis function of a certain dimensionality. In this work, we
provide an optimized one-dimensional FMM for the Cauchy kernel:
\begin{align}
  \label{eq:cauchy-kernel}
  \cauchy(y, x) = \frac{1}{y - x}.
\end{align}
Implementing an FMM requires the derivation of regular and singular
factorizations the kernel function (so-called $S$-factorizations and
$R$-factorizations), along with derivations of translation operators
which reexpand these factorizations: singular-to-singular,
singular-to-regular, and regular-to-regular translation
operators~\cite{fmm-helmholtz}.

The $R$ and $S$-factorizations of the Cauchy kernel are derived in a
straightforward manner from the Taylor expansion of $\cauchy$. For the
$S$-factorization, we fix $x, y$, and
$\xstar \suchthat \abs{x-\xstar} < \abs{y-\xstar}$ and define
$b_m(x, \xstar) = {(x - \xstar)}^m$ and
$S_m(y - \xstar) = {(y - \xstar)}^{-m-1}$. The point $\xstar$ is
referred to as the expansion center. Then, the $S$-factorization of
$\cauchy$ is given by:
\begin{align}
  \label{eq:S-factorization}
  \cauchy(y, x) = \sum_{m=0}^\infty b_m(x, \xstar) S_m(y - \xstar).
\end{align}
Very similarly, for the $R$-factorization, we fix $x, y$, and
$\xstar \suchthat \abs{y - \xstar} < \abs{x - \xstar}$ and define
$a_m(x, \xstar) = -{(x - \xstar)}^{-m-1}$ and
$R_m{(y - \xstar)}^m = {(y - \xstar)}^m$, giving us:
\begin{align}
  \label{eq:R-factorization}
  \cauchy(y, x) = \sum_{m=0}^\infty a_m(x, \xstar) R_m(y - \xstar).
\end{align}
These expansions of $\cauchy$ are used in the implementation of our FMM,
as well as in our discussion of error bounds.

The FMM involves evaluating a function which is comprised of a sum of
weighted kernels at a set of target points. In doing so, the
computational domain is decomposed using a spatial data structure
(e.g.\ a binary tree or octree). The algorithm first expands each
weighted kernel using an $S$-factorization, then applies the
translation operators according to the spatial decomposition
(Figure~\ref{fig:fmm}). We denote the singular-to-singular translation
matrix by $\SSmat$. Represented as an infinite matrix, its entries are
given by:
\begin{align}
  \label{eq:SSmat}
  \parens{\SSmat}_{n,m} \defd \begin{cases} \frac{{(-1)}^{n-m} n! \delta^{n-m}}{(n-m)!m!} & \mbox{if } n \geq m, \\
    0 & \mbox{otherwise,}
  \end{cases}
\end{align}
where $\delta$ is the translation vector between the two expansion
centers. That is, if the resultant $S$-factorization is to be expanded
about $\xstar'$, then $\delta = \xstar' - \xstar$. The corresponding
$S$-reexpansion is:
\begin{align}
  \label{eq:SSmat-reexpansion}
  S_m(y - \xstar) = \sum_{n=0}^\infty \parens{\SSmat}_{n,m} S_n(y - \xstar').
\end{align}
Such a reexpansion requires that $\abs{\delta} < \abs{y -
  \xstar'}$. More details can be found in the appendix, but the
expressions for the singular-to-regular translation operator $\SRmat$
and the regular-to-regular translation operator $\RRmat$ are given by:
\begin{align}
  \label{eq:SRmat}
  \parens{\SRmat}_{n,m} \defd {\parens{-1}^n \parens{m + n}! \over m! n! \delta^{m+n+1}},
\end{align}
and:
\begin{align}
  \label{eq:RRmat}
  \parens{\RRmat}_{n,m} = \begin{cases}
    \frac{m! \delta^{m - n}}{(m - n)!n!} & \mbox{if } n \leq m, \\
    0 & \mbox{otherwise},
  \end{cases}
\end{align}
respectively.

\subsection*{Error Analysis}

The FMM breaks the summation into two parts---a term due to direct
summation and a term due to indirect summation. The direct summation
incurs only the usual error associated with carrying out computations
on a computer (e.g.\ floating point error). The indirect summation
operates by first creating truncated $S$-expansions for each source
point, and then carrying out a series of translations. Typically,
there will be a sequence of $\SSmat$ translations, followed by one
$\SRmat$ translation, and then a sequence of $\RRmat$
translations. The details of the algorithmic procedure can be found
elsewhere, e.g.~\cite{fmm-orig}. We are mainly concerned with deriving
a rough bound for the error due to the indirect summation.

\begin{proof}
  The $\truncnum$ term $S$-expansion of $\cauchy(y, x)$ can be
  written:
  \begin{align}
    \label{eq:S-expansion}
    \cauchy(y, x) = \Serror + \frac{1}{y - \xstar} \sum_{\powerindex=0}^{\truncnum-1} \parens{\frac{x - \xstar}{y - \xstar}}^\powerindex,
  \end{align}
  where $\Serror$ is evaluated to be:
  \begin{align}
    \label{eq:S-error}
    \Serror = \frac{1}{y - x} \parens{\frac{x - \xstar}{y - \xstar}}^\truncnum,
  \end{align}
  by evaluating the corresponding geometric series and invoking
  (\ref{eq:S-error-condition}). Making use of (\ref{eq:S-error}) lets
  us bound:
  \begin{align}
    \label{eq:S-error-bound}
    |\Serror| = |x - y|^{-1} \abs{\frac{x - \xstar}{y - \xstar}}^\truncnum < \frac{1}{R - r} \parens{\frac{r}{R}}^\truncnum,
  \end{align}
  where the inequality follows from (\ref{eq:S-error-condition}).
\end{proof}

\begin{lemma}\label{lemma:SS-error}
  Applying an $\SSmat$ translation operator to a $\truncnum$ term
  approximation of an $S$-factorization of $\cauchy$ incurs no
  additional error.
\end{lemma}

\begin{proof}
  Our truncated $S$-expansion is given by (\ref{eq:S-expansion}),
  which is expanded about the expansion center $\xstar$. Reexpansion
  about $\xstar'$ (\TODO\ what is required of $\xstar'$?) is given by:
  \begin{align}\label{eq:SS-reexpansion}
    \cauchy(y, x) = \Serror + \sum_{n=0}^{\truncnum-1} \parens{\sum_{\powerindex=0}^{\infty} {(\SSmat)}_{n,\powerindex} b_{\powerindex}(x, \xstar)} S_n(y - \xstar')
  \end{align}
  Denoting the error due to $\SS$ translation $\SSerror$, we have:
  \begin{align}\label{eq:SS-error}
    \SSerror = \sum_{n=0}^{\truncnum-1} \parens{\sum_{\powerindex=\truncnum}^\infty {(\SSmat)}_{n,\powerindex} b_\powerindex(x, \xstar)} S_n(x, \xstar'),
  \end{align}
  so that (\ref{eq:SS-reexpansion}) can be written:
  \begin{align}
    \cauchy(y, x) = \Serror + \SSerror + \sum_{n=0}^{\truncnum-1} \parens{\sum_{\powerindex=0}^{\truncnum-1} {(\SSmat)}_{n,\powerindex} b_{\powerindex}(x, \xstar)} S_n(y - \xstar').
  \end{align}
  Now, we note that the entries of $\SSmat$ are only nonzero if
  $n \geq m$. This implies that each entry of $\SSmat$ involved in the
  double summation defining $\SSerror$ is zero, so that
  $\SSerror = 0$. This proves the claim.
\end{proof}

\begin{lemma}\label{lemma:SR-error}
  \TODO\ $\SRmat$ reexpansion error.
\end{lemma}

\begin{lemma}\label{lemma:RR-error}
  \TODO\ $\RRmat$ reexpansion error.
\end{lemma}

% Local Variables:
% TeX-master: "../paper.tex"
% indent-tabs-mode: nil
% End:
