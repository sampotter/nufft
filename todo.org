* Todo
** Immediate todo
   - [ ] Replace one of the waveforms with random combinations of
     sinusoids
   - [ ] Build on a Linux machine and see what the output is like
     (i.e. if we're getting weird push_back slow path stuff)
   - [ ] Count the number of translations (and initial multipole
     expansions) that are being done and see if our method /should/
     beat it (before implementing it...)
   - [ ] Look at the allocators that AA presented in that C++ talk and
     see if using e.g. a stack allocator might make it easier to deal
     with the coefficient dictionaries
   - [ ] Optimize/templatize bookmarks
	 - [ ] We're keeping all of the coefficients that we've ever
       computed in memory---but I think we only need to keep a level
       at a time, since evaluation only uses the last level?
   - [ ] Implement and use my adaptive FMM (as a template parameter)
   - [X] Remove crossed out TODO items from paper
   - [X] Remove checkpoint distribution section
   - [X] Remove error analysis section
   - [X] Numerical results section
	 - [X] Explanation of test functions
	 - [X] Add formulae for test functions
	 - [X] Compare linf error plots (point out different regimes)
	 - [X] Reference methods that we compare with
	 - [X] Discuss timing results
   - [X] Figure out why we shouldn't compare w/ Greengard
   - [X] Conclusion
	 - [X] Add a summary of results
	 - [X] Future work:
	   - [X] Constant-Q transform
	   - [X] Stencil for adaptive FMM/FMRI
	   - [X] Qi Hu's approach to heterogeneous FMM
	   - [X] Chebyshev basis instead of monomial basis
		 - [X] Find reference
	   - [X] Multiple dimensions
   - [X] NUFFT section
	 - [X] Change summation limits to use macro
	 - [X] Add sinc/Dirichlet kernel formulation of interpolation
       problem
   - [X] Add analytic approach to evaluating c0
	 - [X] To implementation; get it working
	 - [X] To writeup
   - [X] Add periodic summation necessary conditions to paper
	 - [X] Try and simplify error bound (both the expression and the
       derivation)
	 - [X] Write it up
	 - [X] Move 3D plot in with these guys and point to it
   - [ ] Complexity analysis
	 - [ ] Edit and add FMM complexity from writeup
	 - [ ] Do NUFFT complexity
     - [ ] Discuss optimal L results
   - [ ] Computing of c_m, m > 0
	 - [ ] Try this approach: since inverting V is a poly. fitting
       problem on [-pi, pi], we could use the corresponding Chebyshev
       nodes to fit our polynomial. This would require evaluating
       phinear at these nodes, but this isn't much overhead---we could
       probably design a sort/unsort to deal with this relatively
       easily. Need to bear in mind where we're actually evaluating
       phinear (should be something like pi*chebnode + pi---verify in
       Mathematica).
	 - [ ] Add plot comparing different approaches. Specifically, if
       the previous approach works and solves our problems, plot on a
       log axis w/ domain [0, 2pi), abs diff between ground truth and
       (for fixed n, e.g. n = 4):
	   - [ ] The approach
	   - [ ] Least squares fitting as suggested in paper, with nodes
         drawn from each of the individual radial neighborhood cells
	   - [ ] Coefficients computed using Remez algorithm
	 - [ ] Make note about why Remez algorithm is unsuitable
   - [ ] Prove things in appendix
   - [ ] Replace square or triangle with some complex test
     function? (check Fourier book)
   - [ ] Optimize c0 implementation
	 - [ ] derive
	 - [ ] implement
	 - [ ] writeup
   - [ ] FMM plot
	 - [ ] Remove dot-dot-dots on either side of the graphic.
	 - [ ] Redo style of FMM TikZ plot using pgfplot colors/design
	 - [ ] Label:
	   - [ ] Translation operators
	   - [ ] Direct summation bins
	   - [ ] Far summation bins
** Next todo
   - [ ] Figure out if we can analytically evaluate c0
	 - [ ] Debug the analytic evaluation
	 - [ ] Write it up
   - [ ] Get rid of K parameter
   - [ ] Figure out if we're missing out on anything by not using
     autotools---can CMake deduce the same flags for optimization?
   - [ ] Remove use of boost::optional
   - [ ] Make bookmarks_t a template argument
   - [ ] Make "linspace bookmarks" for linearly spaced sources
	 - N.B.: could use this for evaluating the checkpoints,
       too... might be worth looking into
   - [X] Fix error plots 
   - [X] Add collocation to C++
	 - [X] Add c0
	 - [X] Add cm
	   - [X] Try p-sized C instead of (p+1)-sized C in nufft.impl.hpp
	   - [X] Try num_cps checkpoints... k
	   - [X] Get it to work
   - [X] Check stuff in
   - [X] Print drafts and collect thoughts
   - [X] Write up why checkpoint method works (i.e. related to Nyquist
     and mean of bandlimited function and why using these particular
     points makes the most sense)
   - [ ] Find fast way to merge checkpoints into target points
	 - and then make sure timings are improved
   - [ ] Make NUFFT use correct L
   - [ ] Make note about how points in numerical tests were chosen:
     i.e. uniformy distributed on [0, 2pi).
   - [ ] Add references in paper to other definitions (i.e. (38) in
     Fessler)
	 - [ ] Rewrite section to use IDFT consistent with (38)
   - [ ] Read up on how the Greengard method works and hand-wave about
     why we don't need to compare with it
   - [ ] Make sure all the libraries I'm comparing with are built the
     same:
	 - [ ] Singlethreaded
	 - [ ] Same compiler flags
	 - [ ] Build Greengard library myself for timing purposes
   - [ ] Add reference to that sinc interpolation/bandlimited note!
   - [ ] elpy
   - [ ] Test [-N/2, ..., N/2) with [0, ..., N) but for my test funcs
   - [ ] Use -⌊K/2⌋, ..., ⌈K/2⌉ instead of
   - [X] Move timer into C++ timer wrapper for timing NUFFT C++
   - [ ] Make sure no multiplications in nufft.impl.hpp are
     inefficient std::complex multiplications
   - [ ] Correct reference for [[http://www.embedded.com/design/real-time-and-performance/4007256/Digital-Signal-Processing-Tricks--Fast-multiplication-of-complex-numbers][this citation]]
   - [ ] Figure out why we're getting weird errors with NFFT
   - [ ] Get rid of old Python dir
   - [ ] Find journals to submit to
	 - [ ] Get LaTeX templates---deal with all that...
   - [ ] Get Makefile to rebuild all of this plotting stuff correctly
** NUFFTs to add
   - [ ] The Julia NUFFT
   - [ ] The Airforce Research Lab (?) NUFFT
   - [ ] Are there any NUFFT packages on CRAN?
** NUFFT
   - [ ] Make Python NUFFT use complex numbers with its
     FMM... (weights and output values)
   - [X] Add C++ NUFFT to nufft.py
   - [ ] Add least squares collocation to C++ NUFFT
   - [ ] Get C++ unit tests working
   - [ ] Add unit test for C++ NUFFT in nufft.py
   - [ ] Figure out what the invalid value in np.multiply is
   - [ ] Get FMM to use new c0 estimation method
   - [ ] Change least squares to upper triangular approach
   - [ ] Another optimization to try:
	 - Using two FMMs may be slower. To get around this, here are two
       ideas:
	   - Choose optimal L separately for nodes and checkpoints
	   - Try this algorithm:
		 1. Create checkpoints in sorted order
		 2. Iterate over nodes, insert checkpoints
		 3. Use a stable sort to unsort (i.e. let nodes = 0 and cps =
            1 or something like this)
** Plots
   - [X] Remove greengard from error plots
   - [ ] Potts' error vs bandlimit to figure out what the
     weirdness is all about
	 - [ ] For different test functions
   - [X] Time vs. problem size vs. method (fixed error)
   - [X] Error vs. problem size vs. method (fixed time)
	 - [X] ℓ∞
	 - [X] ℓ₂
   - [ ] Plot of error bound in terms of L and p vs plot of actual
     error in terms of L and p (use scatter plot markers for some kind
     of threshold... e.g. choice of L for each p guaranteeing some
     error threshold)
   - [-] Correct digits vs. radians vs. method (fixed... what?)
	 - [X] It looks like this actually varies fairly substantially for
       different test series—so, include plots for different test
       series!
	 - [ ] Fix xticks
   - [X] Problem size vs. optimal L (3D plot?)
** Analysis
   - [ ] Figure out optimal P-by-P fitting matrix for upper triangular
     approach using explicit forms that we have...
   - [ ] Figure out method of choosing correct FMM parameters
** Cauchy kernel stuff
   - [ ] Find paper Nail was talking about re: Chebyshev expansions
	 - [ ] Implement using C++ framework
   - [ ] Try applying Gram-Schmidt to 1/(x-y)
	 - [ ] Implement using C++ framework
   - [ ] See if there were other methods of representing the Cauchy
     kernel discussed in the slides in class
* NUFFT
** Stuff to Code
   - [ ] Forward transform.
   - [ ] Vandermonde inversion for collocation.
** Analysis
   - [ ] Compare methods of estimating c0.
   - [ ] Are any of the test functions more or less difficult to interpolate?
	 - [ ] What about adding test functions which are just random sums
       of sinusoids?
** Plots to Make
*** Comparison Plots
**** Problem Size vs. Time (to compute to some prescribed accuracy)
	 Things to compare:
	 - IFFT (baseline)
	 - INUFFT using just phifar (note how large it needs to be to
       achieve the required accuracy and if this changes depending on
       the problem size)
	 - INUFFT using phifar and phinear
	 - min/max INUFFT
	 Note: /since clang on OS X doesn't support OpenMP, parfor loops
	 are treated as for loops./
** Goals
   - Approximate good choices for:
	 + the FMM depth,
	 + the truncation number,
	 + the radial neighborhood size,
	 + and the number of checkpoint pairs,
	 based on:
	 + the bandlimit,
	 + the function evaluates,
	 + and the evaluation points.
** Things to Verify Using Plots
   - [ ] Increasing the size of the radial neighborhood, all else being
     equal, should improve the accuracy.
   - [ ] Different checkpoint methods should perform differently.
   - [ ] Increasing the truncation number should improve accuracy.
   - [ ] Increasing the FMM depth should decrease accuracy but improve
     speed.
   - [ ] With the rest of the parameters fixed, there should be an
     optimal FMM depth.
** Things to Read
   - [ ] [[https://en.wikipedia.org/wiki/Dirichlet_kernel][Wikipedia - Dirichlet kernel]]
   - [ ] "Multipole Expansions and Pseudospectral Cardinal Functions"
** Links
*** References
   	- [[http://fastmultipole.org/Main/T-NuFFT][site containing links and references to NuFFT implementations]]
*** NUFFT Implementations
   	- [[http://cs.nyu.edu/cs/faculty/berger/nufft/nufft.html][CMCL (Courant)]]
   	- [[https://www-user.tu-chemnitz.de/~potts/nfft/download.php][Potts (TU Chemnitz)]]
   	- [[http://www.mathworks.com/matlabcentral/fileexchange/25135-nufft--nfft--usfft][Matthew Ferrara (Air Force Research Laboratory)]]
   	- [[http://web.eecs.umich.edu/~fessler/code/index.html][Image reconstruction toolbox]]

* Cauchy Kernel FMM
** C++ Implementation
   - [ ] Replace ~std::vector~ with ~boost::numeric::ublas::vector~?
   - [ ] Figure out how to deal with domain and range types -- if this
	 is even necessary...
   - [X] Try out using Boost.Optional for the bookmarks instead of
	 using a pair of -1's to indicate no bookmark.
   - [ ] Alternative bookmark data structures to try:
	 - [ ] Heap-based
	 - [ ] Linear probing (i.e. no buckets) implementation
   - [ ] Play around with the ~inline~ keyword for optimization.
   - [ ] Factor out Kahan summation for reuse...
   - [ ] Kahan product?
   - [ ] Make ~p~ a template parameter to enable use of arrays on the
	 stack...
   - [ ] Look into Shewchuk summation...
*** Refactoring
	- [ ] There are a lot of loops involving indices compared to
      variables which are initially declared in the argument list of
      some function. It would be nice to be able to allow for
      arbitrary types for the indices, which will require those
      arguments to be of some template type, in which case we will
      need to go and replace a lot of things with auto and decltype,
      and MOST LIKELY provide some separate---conditionally
      compiled---sections of code for dealing with signed and unsigned
      cases...
*** Optimization
**** General
   	 - [ ] Put EVERYTHING on the stack/or preallocate all memory used
   	 - [ ] Remove dependance on boost (for compilation speed)
   	 - [ ] Diagram algorithm to try and figure out best way to move memory around
   	 - [ ] Sum directly into coefficient vectors instead of using an intermediate workspace
   	 - [ ] Don't use maps -- or at least don't use STL maps?
   	 - [ ] don't unnecessarily propagate coefficients that aren't there...
   	 - [ ] SSE/SIMD?
   	 - [ ] use a heap for the index finder?
**** Implementation-specific
	 - [X] Coalesce X and X_per (i.e. X is contained in X_per, so it's
       redundant)
	 - [X] Also redundant: computing X_per and then scaling X_per---we
       can compute it directly.
	 - [ ] Look into whether or not computing values of X_per and
       Fas_per on the fly would be more efficient than storing them
       (it would certainly take way less memory)
	 - [ ] Major redundancy with Y, Yc, Yc_tilde.
	 - [ ] Look into using the FMM on Y and Yc/Yc_tilde separately to
       avoid the overhead of sorting.
	 - [ ] Look into going back to -1's instead of
       boost::optional... Only problem here, though, is that if we
       want to support unsigned ints, this will be problematic.
	   - [ ] In order to support unsigned types, we could encode "not
         there" as a nonsense value---i.e. if we require first <=
         second, then choosing some value (e.g. (1, 0)) s.t. first >
         second would encode "not there".
     - [ ] Add more SFINAE overloads for addition and multiplication
       (accumulation-style)
     - [ ] Use SFINAE overloads throughout cauchy.impl.hpp
     - [ ] Try [[http://stackoverflow.com/questions/4638473/how-to-powreal-real-in-x86][this answer]] out in cauchy.impl.hpp—will need to figure
       out a way to conditionally compile code depending on if the
       instruction set is X86
**** Future optimizations
	 - [ ] Compute checkpoint FMM directly if there are few enough checkpoints?
	 - [ ] We could also use a different FMM (i.e. in terms of
       truncation number and level) for the checkpoints. Since they
       are more sparsely distributed, we may want to use a shallower
       FMM...?
**** Eventually
   	 - [ ] parallelize
	   - OpenMP?
	   - C++11 threads?
	   - TBB?
**** Things to try autotuning
   	 - [ ] Whether or not functions are inline
   	 - [ ] Duff's device for loop unrolling (this can be done using
       TMP--Game Programming Gems (maybe #1) books has a good
       tutorial).
   	 - [ ] Loop tiling
